Metadata-Version: 2.1
Name: find_keyword_xtvu
Version: 5.6.7
Summary: A package to find keywords in .pdf, .docx, .odt, and .rtf files, with support for multiple languages and the ability to run on multiple CPU cores
Author: Xuan Tung VU
Classifier: Programming Language :: Python :: 3
Classifier: Operating System :: OS Independent
Classifier: License :: OSI Approved :: MIT License
Classifier: Development Status :: 5 - Production/Stable
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: End Users/Desktop
Classifier: Intended Audience :: Education
Classifier: Intended Audience :: Science/Research
Classifier: Topic :: Software Development :: Libraries :: Python Modules
Requires-Python: >=3.10
Description-Content-Type: text/markdown
License-File: LICENSE.txt
Requires-Dist: pdfplumber
Requires-Dist: pytesseract
Requires-Dist: pandas
Requires-Dist: Pillow
Requires-Dist: spacy
Requires-Dist: openpyxl
Requires-Dist: pdf2image
Requires-Dist: reportlab
Requires-Dist: pypandoc
Requires-Dist: python-docx
Requires-Dist: requests
Requires-Dist: packaging
Requires-Dist: setuptools>=40.0.0

## ‚ú® What's New in Version 5.6.7
- **üîß Bug Fix**: Fixed the error in generating contingency tables and calculating occurrences of keywords in a sentence.
## ‚ú® What's New in Version 5.6.6
- **üöÄ Enhanced Keyword Processing**: The library now supports the identification and handling of synonymous keywords. You can search for individual keywords or groups of synonymous keywords, and they will all be treated equivalently, ensuring a more flexible and accurate search process.
## ‚ú® What's New in Version 5.6.4
- **üöÄ Improvement in the way documents are read**: Enhanced the document reading methods to produce more accurate results.

## ‚ú® What's New in Version 5.6.2
- **üÜï New argument `exact_match`**: Enables precise matching of the keyword during text extraction. If set to `False`, the matching will be based on the lemma form, allowing for more flexible searches and capturing different forms of the word.
- **üÜï New modification to `threads_rest`**: To prevent potential blocking on certain computers, the default number of cores used by this code is now set to **half of the available cores**, ensuring smoother operation without overloading the system. ***If your computer is lagging significantly, you may want to increase this number to help resolve the issue.***
## üõ†Ô∏è What's New in Version 5.6.1
- **üîß Bug Fix**: Minor bug fixes to enhance stability.

## ‚ú® What's New in Version 5.6
- **üÜï New argument `lang_OCR_tesseract`**: Add the ability to specify the language of the Tesseract OCR model for extracting text from images.

## üõ†Ô∏è What's New in Version 5.5.9
- **üîß Bug Fix with `poppler_path`**: Solves the issue with "Unable to get page count. Is poppler installed and in PATH?" by allowing you to specify the Poppler installation path.

## ‚ú® What's New in Version 5.5.8
- **üÜï New argument `use_tesseract`**: Adds an option to use Tesseract OCR for extracting text from images within documents. When set to `False`, only text is processed without OCR.

## üõ†Ô∏è What's New in Version 5.5.7
- **üîß Bug Fix**: Minor bug fixes to enhance stability.

## üõ†Ô∏è What's New in Version 5.5.6
- **üîß Bug Fix**: Resolved the `tesseract` not found issue.
- **üîÑ Update**: All log messages are now standardized in English.

## üõ†Ô∏è What's New in Version 5.5.5
- **üîß Bug Fix**: Corrected an issue with calculating contingency tables of keyword frequency when `fusion_keyword_before_after = True`.

## üõ†Ô∏è What's New in Version 5.5.4
- **üîß Bug fix**: Fixed an issue where some document names couldn't be read correctly.
- **üÜï New argument `fusion_keyword_before_after`**: Introduces the ability to merge phrases to avoid redundancy in the results.

## üåç What's New in Version 5.5.2
- **üåê Multilingual Support**: This version now includes support for multiple languages by integrating SpaCy's NLP models. You can now search for keywords and extract sentences in languages such as English, French, German, Spanish, and more. The supported models are listed in the [SpaCy documentation](https://spacy.io/usage/models).

# English version
The `find_keyword_xtvu` Python package facilitates the search for keywords across **PDF, DOCX, ODT, and RTF** files, enabling the extraction of sentences that contain these keywords. It also offers support for **multiple languages** and can run on **multicore CPUs**.






## Installation

You can install this package via pip:

```bash
pip install find-keyword-xtvu==<latest_version_on_PyPi>
```

## Directory Structure

The directory organization containing the .py code and documents can be structured as follows:
```
/Parent Folder
‚îÇ
‚îú‚îÄ‚îÄ script_principal.py     # The main Python script
‚îÇ
‚îú‚îÄ‚îÄ fichiers_entre          # Folder containing subfolders of PDF files
‚îÇ   ‚îú‚îÄ‚îÄ files1              # Subfolder containing input .pdf, .docx, .otd, and .rtf files
‚îÇ   ‚îú‚îÄ‚îÄ files2          
‚îÇ   ‚îú‚îÄ‚îÄ files3          
‚îÇ   ...
‚îî‚îÄ‚îÄ resultats               # Folder containing the results
```

## Usage

1. **Place the files in the input directory**:
   - Place the PDF, DOCX, ODT, or RTF files you want to analyze into the subfolders within the `fichiers_entree` directory. By default, you can organize them in a single subfolder (e.g., files1) or in multiple subfolders (files2, files3, etc.), depending on your needs.
   
2. **Define the keywords**:
   - Open the `script_principal.py` script and modify the `KEYWORDS` list to include the keywords you want to search for in the files.
   
3. **Run the script**:
   - Run the `script_principal.py` script in an IDE like Visual Studio Code.

The `script_principal.py` file uses the `find_keyword_xtvu` package and can be organized as follows:

```python
from find_keyword_xtvu import find_keyword_xtvu
if __name__ == "__main__":
    find_keyword_xtvu(
        prefixe_langue = 'fr',
        threads_rest = None,
        nb_phrases_avant = 10,
        nb_phrases_apres = 10,
        keywords = [["",""],""],
        exact_match = True,
        taille = 20,
        timeout = 200,
        result_keyword_table_name = "",
        freque_document_keyword_table_name = "",
        fusion_keyword_before_after = False,
        tesseract_cmd = "/usr/local/bin/tesseract",
        use_tesseract = False,
        lang_OCR_tesseract = 'fra',
        input_path = "/path/to/fichiers_entre",
        output_path = "/path/to/resultats",
        poppler_path = ""
    )

```


## Arguments
- `prefixe_langue`: Language prefix to specify the language model to use (default value: `'fr'`). To know the supported languages and their prefixes, see the [SpaCy documentation](https://spacy.io/usage/models). If you provide an unsupported prefix, or if you want to use the multilingual model, specify the `multi` argument. In either case, the multilingual model `xx_ent_wiki_sm` will be used. Learn more about this model [here](https://spacy.io/models/xx).
- `threads_rest`: Number of threads reserved for other tasks. If left unspecified, the library will default to using half of the available cores.
- `nb_phrases_avant`: Number of sentences to include before the keyword (default value: `10`).
- `nb_phrases_apres`: Number of sentences to include after the keyword (default value: `10`).
- `keywords`: List of keywords to search for. To group multiple keywords that should be treated the same way, use a list format like `["keyword1", "keyword2"]` (for example, `["local", "nearby"]` to treat them as synonyms). If you want to treat keywords separately, simply place each keyword as a standalone string, like `"keyword"`. For example, if we structure it like this `[["local", "nearby"], "school", "library", "park"]`, it means that "local" and "nearby" will be treated as synonyms, while "school", "library", and "park" will each be treated as distinct keywords.
- `exact_match`: This boolean parameter determines whether the keyword search should match the exact word as provided. If set to `True`, only exact matches of the keyword will be considered during text extraction. If set to `False`, the search will be based on the lemma form of the keyword, allowing for more flexible matching by capturing different variations of the word (e.g., "run," "running," "ran"). To learn more about lemma forms, see [this page](https://spacy.io/usage/linguistic-features).
- `taille`: Maximum file size to process in megabytes (default value: `20` MB).
- `timeout`: Maximum time for processing a page in seconds (default value: `200`).
- `result_keyword_table_name`: Name of the table for keyword results. If this field is empty, a default name for this table will be `res`.
- `freque_document_keyword_table_name`: Name of the table for the results of the contingency tables of keyword frequency in each file folder. If this field is empty, the default name for this table will be `freque_document_keyword`.
- `fusion_keyword_before_after`: This boolean parameter controls whether the function should avoid including redundant phrases when a keyword appears multiple times within close proximity in the text. When set to `True`, the function ensures that phrases surrounding a keyword are only extracted once, even if they overlap with the phrases surrounding another occurrence of the same keyword. This prevents the repetition of phrases in the final output, leading to a more concise result. If set to `False`, the function will extract all phrases surrounding each occurrence of the keyword, which may lead to redundancy if the keyword appears frequently in the text. (default value: `False`)
- `tesseract_cmd`: Path to the Tesseract executable (default value: `"/usr/local/bin/tesseract"`).
- `use_tesseract`: This boolean parameter controls whether to use Tesseract OCR for processing images within the documents. When set to `True`, the function will extract text from images using Tesseract OCR, which is useful if the documents contain scanned images or embedded pictures with text. Ensure that `tesseract_cmd` is correctly set to the path of the Tesseract executable. If set to `False`, the function will ignore images in the documents and only process the text that is directly extractable without OCR. This can be faster and avoids the need to install Tesseract. (default value: `False`)
- `lang_OCR_tesseract`: Language code used by Tesseract OCR to identify the language of the text to be extracted from images. You can specify other language codes supported by Tesseract depending on the language of the text in the images. For a complete list of supported languages and their codes, you can consult the [official Tesseract documentation](https://github.com/tesseract-ocr/tesseract/blob/main/doc/tesseract.1.asc#languages). (default value: `fra`)
- `input_path`: Path to the folder containing the files to be processed.
- `output_path`: Path to the folder where the results will be saved.
- `poppler_path`: You need to install Poppler by following the instructions on this [link](https://pdf2image.readthedocs.io/en/latest/installation.html). After installing Poppler, specify the path to the `bin` directory inside the Poppler installation using the `poppler_path` argument in your code. This ensures the library can access the necessary binaries to process PDFs correctly, especially on systems where Poppler isn't in the system's PATH. On macOS, in most cases, you can set `poppler_path` to `/usr/local/opt/poppler/bin`.



## Outputs

The `find_keyword_xtvu` function will generate the following three Excel workbooks (.xlsx):
1. A file containing the results of the keywords found in the documents, with a name that can be defined by the `result_keyword_table_name` argument in the `find_keyword_xtvu` function.
2. A file containing the contingency tables of keyword frequency in the documents, with a name that can be defined by the `freque_document_keyword_table_name` argument in the `find_keyword_xtvu` function. Each contingency table shows how many times each keyword was found in each document within a specific folder. These tables are saved in different sheets within a single Excel workbook, with each sheet representing a folder.
3. A file listing problematic files, named `heavy_or_slow_df.xlsx`.


## Contribution
As the author of this library, I would like to thank Madame Sylvie HUET, researcher at LISC, INRAE, Centre Clermont-Auvergne-Rh√¥ne-Alpes, France, for her valuable contributions.

Contributions are welcome! If you would like to improve this project or if you have any questions, feel free to contact me at vuxuantung09134@gmail.com (in French, English, or Vietnamese).

## License

This project is licensed under the MIT License. See the `LICENSE` file for details.

# Version Fran√ßaise

Le package Python `find_keyword_xtvu` facilite la recherche de mots-cl√©s dans les fichiers **PDF, DOCX, ODT et RTF**, permettant d'extraire les phrases contenant ces mots-cl√©s. Il offre √©galement un support pour **plusieurs langues** et peut s'ex√©cuter sur des **CPU multic≈ìurs**.

## Installation

Vous pouvez installer ce package via pip :

```bash
pip install find-keyword-xtvu==<derni√®re_version_sur_PyPi>
```

## Structure du R√©pertoire

L'organisation du dossier contenant le code .py et les documents peut √™tre structur√©e comme suit :
```
/Dossier parent
‚îÇ
‚îú‚îÄ‚îÄ script_principal.py     # Le script Python principal
‚îÇ
‚îú‚îÄ‚îÄ fichiers_entre          # Dossier contenant les sous-dossiers de fichiers PDF
‚îÇ   ‚îú‚îÄ‚îÄ files1              # Sous-dossier contenant les fichiers .pdf, .docx, .otd et .rtf d'entr√©e
‚îÇ   ‚îú‚îÄ‚îÄ files2          
‚îÇ   ‚îú‚îÄ‚îÄ files3          
‚îÇ   ...
‚îî‚îÄ‚îÄ resultats               # Dossier contenant les r√©sultats
```

## Utilisation

1. **Placez les fichiers dans le r√©pertoire d'entr√©e** :
    - Mettez les fichiers PDF, DOCX, ODT, ou RTF que vous souhaitez analyser dans les sous-dossiers du dossier `fichiers_entree`. Par d√©faut, vous pouvez les organiser dans un seul sous-dossier (par exemple, `files1`) ou dans plusieurs sous-dossiers (`files2`, `files3`, etc.), selon vos besoins.
    
2. **D√©finissez les mots-cl√©s** :
   - Ouvrez le script `script_principal.py` et modifiez la liste `KEYWORDS` pour inclure les mots-cl√©s que vous souhaitez rechercher dans les fichiers.
   
3. **Ex√©cutez le script** :
   - Ex√©cutez le script `script_principal.py`dans un IDE comme Visual Studio Code.

Le fichier `script_principal.py` utilise le package `find_keyword_xtvu` et peut √™tre organis√© comme suit :

```python
from find_keyword_xtvu import find_keyword_xtvu
if __name__ == "__main__":
    find_keyword_xtvu(
        prefixe_langue = 'fr',
        threads_rest = None,
        nb_phrases_avant = 10,
        nb_phrases_apres = 10,
        keywords = [["",""],""],
        exact_match = True,
        taille = 20,
        timeout = 200,
        result_keyword_table_name = "",
        freque_document_keyword_table_name = "",
        fusion_keyword_before_after = False,
        tesseract_cmd = "/usr/local/bin/tesseract",
        use_tesseract = False,
        lang_OCR_tesseract = 'fra',
        input_path = "/path/to/fichiers_entre",
        output_path = "/path/to/resultats",
        poppler_path = ""
    )
```


## Arguments
- `prefixe_langue` : Pr√©fixe de langue pour sp√©cifier le mod√®le linguistique √† utiliser (valeur par d√©faut : `'fr'`). Pour conna√Ætre les langues support√©es et leurs pr√©fixes, consultez la [documentation SpaCy](https://spacy.io/usage/models). Si vous fournissez un pr√©fixe non support√©, ou si vous souhaitez utiliser le mod√®le multilingue, sp√©cifiez l'argument `multi`. Dans les deux cas, le mod√®le multilingue `xx_ent_wiki_sm` sera utilis√©. En savoir plus sur ce mod√®le [ici](https://spacy.io/models/xx).
- `threads_rest` : Nombre de threads r√©serv√©s pour d'autres t√¢ches. Si non sp√©cifi√©, la biblioth√®que utilisera par d√©faut la moiti√© des c≈ìurs disponibles.
- `nb_phrases_avant` : Nombre de phrases √† inclure avant le mot-cl√© (valeur par d√©faut : `10`).
- `nb_phrases_apres` : Nombre de phrases √† inclure apr√®s le mot-cl√© (valeur par d√©faut : `10`).
- `keywords` : Liste de mots-cl√©s √† rechercher. Pour regrouper plusieurs mots-cl√©s qui doivent √™tre trait√©s de la m√™me mani√®re, utilisez un format de liste comme `["mot1", "mot2"]` (par exemple, `["local", "proche"]` pour les traiter comme des synonymes). Si vous souhaitez traiter les mots-cl√©s s√©par√©ment, placez chaque mot-cl√© sous forme de cha√Æne de caract√®res individuelle, comme `"mot"`. Par exemple, si nous structurons la liste ainsi `[["local", "proche"], "√©cole", "biblioth√®que", "parc"]`, cela signifie que "local" et "proche" seront trait√©s comme des synonymes, tandis que "√©cole", "biblioth√®que" et "parc" seront chacun trait√©s comme des mots-cl√©s distincts.
- `exact_match` : Ce param√®tre bool√©en d√©termine si la recherche du mot-cl√© doit correspondre exactement au mot fourni. S'il est r√©gl√© sur `True`, seules les correspondances exactes du mot-cl√© seront prises en compte lors de l'extraction du texte. S'il est r√©gl√© sur `False`, la recherche sera bas√©e sur le lemme du mot-cl√©, ce qui permet une correspondance plus flexible en capturant diff√©rentes variations du mot (par exemple, "courir", "courant", "couru"). Pour en savoir plus sur les formes de lemmes, consultez [cette page](https://spacy.io/usage/linguistic-features).
- `taille` : Taille maximale des fichiers √† traiter en m√©gaoctets (valeur par d√©faut : `20` MB).
- `timeout` : Dur√©e maximale pour le traitement d'une page en secondes (valeur par d√©faut : `200`).
- `result_keyword_table_name` : Nom de la table pour les r√©sultats des mots-cl√©s. Si ce champ est vide, un nom par d√©faut pour cette table sera `res`.
- `freque_document_keyword_table_name` : Nom de la table pour les r√©sultats des tables de contingence de la fr√©quence des mots-cl√©s dans chaque dossier de fichiers. Si ce champ est vide, le nom par d√©faut pour cette table sera `freque_document_keyword`.
- `fusion_keyword_before_after` : Ce param√®tre bool√©en contr√¥le si la fonction doit √©viter d'inclure des phrases redondantes lorsque un mot-cl√© appara√Æt plusieurs fois √† proximit√© dans le texte. Lorsqu'il est d√©fini sur `True`, la fonction garantira que les phrases entourant un mot-cl√© sont extraites une seule fois, m√™me si elles chevauchent les phrases entourant une autre occurrence du m√™me mot-cl√©. Cela emp√™che la r√©p√©tition de phrases dans le r√©sultat final, conduisant √† un r√©sultat plus concis. Si d√©fini sur `False`, la fonction extraira toutes les phrases entourant chaque occurrence du mot-cl√©, ce qui peut conduire √† une redondance si le mot-cl√© appara√Æt fr√©quemment dans le texte. (valeur par d√©faut : `False`)
- `tesseract_cmd` : Chemin vers l'ex√©cutable Tesseract (valeur par d√©faut : `"/usr/local/bin/tesseract"`).
- `use_tesseract` : Ce param√®tre bool√©en d√©termine si l'on doit utiliser Tesseract OCR pour traiter les images dans les documents. Lorsqu'il est d√©fini sur `True`, la fonction extraira le texte des images en utilisant Tesseract OCR, ce qui est utile si les documents contiennent des images scann√©es ou des images int√©gr√©es avec du texte. Assurez-vous que `tesseract_cmd` est correctement configur√© pour pointer vers le chemin de l'ex√©cutable Tesseract. Si le param√®tre est d√©fini sur `False`, la fonction ignorera les images dans les documents et traitera uniquement le texte directement extractible sans OCR. Cela peut √™tre plus rapide et √©vite la n√©cessit√© d'installer Tesseract. (valeur par d√©faut : `False`)
- `lang_OCR_tesseract`: Code de langue utilis√© par Tesseract OCR pour identifier la langue du texte √† extraire des images. Par d√©faut, ce param√®tre est r√©gl√© sur `'fra'` pour le fran√ßais. Vous pouvez sp√©cifier d'autres codes de langue pris en charge par Tesseract selon la langue du texte dans les images. Pour une liste compl√®te des langues et de leurs codes support√©s par Tesseract, vous pouvez consulter la [documentation officielle de Tesseract](https://github.com/tesseract-ocr/tesseract/blob/main/doc/tesseract.1.asc#languages).
- `input_path` : Chemin vers le dossier contenant les fichiers √† traiter.
- `output_path` : Chemin vers le dossier o√π les r√©sultats seront enregistr√©s.
- `poppler_path` : Vous devez installer Poppler en suivant les instructions de ce [lien](https://pdf2image.readthedocs.io/en/latest/installation.html). Apr√®s avoir install√© Poppler, sp√©cifiez le chemin vers le r√©pertoire `bin` √† l'int√©rieur de l'installation de Poppler en utilisant l'argument `poppler_path` dans votre code. Cela permet √† la biblioth√®que d'acc√©der aux binaires n√©cessaires pour traiter correctement les fichiers PDF, surtout sur les syst√®mes o√π Poppler n'est pas dans le `PATH` du syst√®me. Sur macOS, vous pouvez g√©n√©ralement d√©finir `poppler_path` sur `/usr/local/opt/poppler/bin`.

## Sorties

La fonction `find_keyword_xtvu` va g√©n√©rer trois classeurs Excel (.xlsx) suivants :
1. Un fichier contenant les r√©sultats des mots-cl√©s trouv√©s dans les documents avec un nom pouvant √™tre d√©fini par l'argument `result_keyword_table_name` dans la fonction `find_keyword_xtvu`.
2. Un fichier contenant les tables de contingence de la fr√©quence des mots-cl√©s dans les documents avec un nom pouvant √™tre d√©fini par l'argument `freque_document_keyword_table_name` dans la fonction `find_keyword_xtvu`. Chaque table de contingence montre combien de fois chaque mot-cl√© a √©t√© trouv√© dans chaque document au sein d'un dossier sp√©cifique. Ces tables sont enregistr√©es sous diff√©rentes feuilles dans un seul classeur Excel, avec chaque feuille repr√©sentant un dossier.
3. Un fichier r√©pertoriant les fichiers probl√©matiques, nomm√© `heavy_or_slow_df.xlsx`.






## Contribution

En tant qu'auteur de cette biblioth√®que, je tiens √† remercier Madame Sylvie HUET, chercheuse au LISC, INRAE, Centre Clermont-Auvergne-Rh√¥ne-Alpes, France, pour ses pr√©cieuses contributions.

Les contributions sont les bienvenues ! Si vous souhaitez am√©liorer ce projet ou si vous avez des questions, n'h√©sitez pas √† me contacter √† l'adresse vuxuantung09134@gmail.com (en fran√ßais, anglais ou vietnamien).

## Licence

Ce projet est sous licence MIT. Voir le fichier `LICENSE` pour plus de d√©tails.
